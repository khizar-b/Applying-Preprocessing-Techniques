{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4874c7f6-b5ae-49f6-8f58-e237f9efd880",
   "metadata": {},
   "source": [
    "# Step 1: Import Necessary Libraries\n",
    "We begin by importing the required libraries, including pandas, numpy, scikit-learn, and TensorFlow/Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "57059fad-7722-435b-ae44-737c954941c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4f6062d-2b57-4a6f-959e-1e07333ecb27",
   "metadata": {},
   "source": [
    "# Explanation\n",
    "1.pandas: A library used for data manipulation and analysis. It provides data structures like DataFrames that are suitable for handling tabular data.\n",
    "\n",
    "2.numpy: A library for numerical operations in Python. It supports large, multi-dimensional arrays and matrices.\n",
    "\n",
    "3.train_test_split: A function from sklearn to split datasets into training and testing sets.\n",
    "\n",
    "4.MinMaxScaler: A feature scaling method that transforms features to a specified range, often [0, 1].\n",
    "\n",
    "5.SimpleImputer: A class from sklearn used for handling missing values by replacing them with a specified statistic (mean, median, mode)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce930139-d11f-4cdf-83df-472e8e8594e4",
   "metadata": {},
   "source": [
    "# Step 2: Load the Spambase Dataset\n",
    "Next, we will load the Spambase dataset, which contains features extracted from emails to classify them as spam or not spam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5a6ec202-3193-4440-b7ca-b9f4ebc025bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/spambase/spambase.data\"\n",
    "column_names = [f'feature_{i}' for i in range(1, 58)] + ['spam_label']\n",
    "\n",
    "df = pd.read_csv(url, header=None, names=column_names)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ebb3a0-6541-451e-abde-6e65510c68d5",
   "metadata": {},
   "source": [
    "# Explanation:\n",
    "We specify the URL for the dataset and define column names, where the last column is labeled spam_label, indicating whether the email is spam (1) or not (0).\n",
    "\n",
    "We use pd.read_csv() to read the dataset into a pandas DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca9f5e46-ec09-4288-a3d4-fd564d8e1ab6",
   "metadata": {},
   "source": [
    "# Step 3: Data Inspection\n",
    "It's essential to inspect the dataset to understand its structure and content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ece3cc1-acb5-457e-9056-22422723463e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   feature_1  feature_2  feature_3  feature_4  feature_5  feature_6  \\\n",
      "0       0.00       0.64       0.64        0.0       0.32       0.00   \n",
      "1       0.21       0.28       0.50        0.0       0.14       0.28   \n",
      "2       0.06       0.00       0.71        0.0       1.23       0.19   \n",
      "3       0.00       0.00       0.00        0.0       0.63       0.00   \n",
      "4       0.00       0.00       0.00        0.0       0.63       0.00   \n",
      "\n",
      "   feature_7  feature_8  feature_9  feature_10  ...  feature_49  feature_50  \\\n",
      "0       0.00       0.00       0.00        0.00  ...        0.00       0.000   \n",
      "1       0.21       0.07       0.00        0.94  ...        0.00       0.132   \n",
      "2       0.19       0.12       0.64        0.25  ...        0.01       0.143   \n",
      "3       0.31       0.63       0.31        0.63  ...        0.00       0.137   \n",
      "4       0.31       0.63       0.31        0.63  ...        0.00       0.135   \n",
      "\n",
      "   feature_51  feature_52  feature_53  feature_54  feature_55  feature_56  \\\n",
      "0         0.0       0.778       0.000       0.000       3.756          61   \n",
      "1         0.0       0.372       0.180       0.048       5.114         101   \n",
      "2         0.0       0.276       0.184       0.010       9.821         485   \n",
      "3         0.0       0.137       0.000       0.000       3.537          40   \n",
      "4         0.0       0.135       0.000       0.000       3.537          40   \n",
      "\n",
      "   feature_57  spam_label  \n",
      "0         278           1  \n",
      "1        1028           1  \n",
      "2        2259           1  \n",
      "3         191           1  \n",
      "4         191           1  \n",
      "\n",
      "[5 rows x 58 columns]\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4601 entries, 0 to 4600\n",
      "Data columns (total 58 columns):\n",
      " #   Column      Non-Null Count  Dtype  \n",
      "---  ------      --------------  -----  \n",
      " 0   feature_1   4601 non-null   float64\n",
      " 1   feature_2   4601 non-null   float64\n",
      " 2   feature_3   4601 non-null   float64\n",
      " 3   feature_4   4601 non-null   float64\n",
      " 4   feature_5   4601 non-null   float64\n",
      " 5   feature_6   4601 non-null   float64\n",
      " 6   feature_7   4601 non-null   float64\n",
      " 7   feature_8   4601 non-null   float64\n",
      " 8   feature_9   4601 non-null   float64\n",
      " 9   feature_10  4601 non-null   float64\n",
      " 10  feature_11  4601 non-null   float64\n",
      " 11  feature_12  4601 non-null   float64\n",
      " 12  feature_13  4601 non-null   float64\n",
      " 13  feature_14  4601 non-null   float64\n",
      " 14  feature_15  4601 non-null   float64\n",
      " 15  feature_16  4601 non-null   float64\n",
      " 16  feature_17  4601 non-null   float64\n",
      " 17  feature_18  4601 non-null   float64\n",
      " 18  feature_19  4601 non-null   float64\n",
      " 19  feature_20  4601 non-null   float64\n",
      " 20  feature_21  4601 non-null   float64\n",
      " 21  feature_22  4601 non-null   float64\n",
      " 22  feature_23  4601 non-null   float64\n",
      " 23  feature_24  4601 non-null   float64\n",
      " 24  feature_25  4601 non-null   float64\n",
      " 25  feature_26  4601 non-null   float64\n",
      " 26  feature_27  4601 non-null   float64\n",
      " 27  feature_28  4601 non-null   float64\n",
      " 28  feature_29  4601 non-null   float64\n",
      " 29  feature_30  4601 non-null   float64\n",
      " 30  feature_31  4601 non-null   float64\n",
      " 31  feature_32  4601 non-null   float64\n",
      " 32  feature_33  4601 non-null   float64\n",
      " 33  feature_34  4601 non-null   float64\n",
      " 34  feature_35  4601 non-null   float64\n",
      " 35  feature_36  4601 non-null   float64\n",
      " 36  feature_37  4601 non-null   float64\n",
      " 37  feature_38  4601 non-null   float64\n",
      " 38  feature_39  4601 non-null   float64\n",
      " 39  feature_40  4601 non-null   float64\n",
      " 40  feature_41  4601 non-null   float64\n",
      " 41  feature_42  4601 non-null   float64\n",
      " 42  feature_43  4601 non-null   float64\n",
      " 43  feature_44  4601 non-null   float64\n",
      " 44  feature_45  4601 non-null   float64\n",
      " 45  feature_46  4601 non-null   float64\n",
      " 46  feature_47  4601 non-null   float64\n",
      " 47  feature_48  4601 non-null   float64\n",
      " 48  feature_49  4601 non-null   float64\n",
      " 49  feature_50  4601 non-null   float64\n",
      " 50  feature_51  4601 non-null   float64\n",
      " 51  feature_52  4601 non-null   float64\n",
      " 52  feature_53  4601 non-null   float64\n",
      " 53  feature_54  4601 non-null   float64\n",
      " 54  feature_55  4601 non-null   float64\n",
      " 55  feature_56  4601 non-null   int64  \n",
      " 56  feature_57  4601 non-null   int64  \n",
      " 57  spam_label  4601 non-null   int64  \n",
      "dtypes: float64(55), int64(3)\n",
      "memory usage: 2.0 MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(df.head())\n",
    "print(df.info())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4439ce8-de55-43c0-a85b-5f3753dd3fac",
   "metadata": {},
   "source": [
    "# Explanation:\n",
    "df.head() displays the first five rows of the dataset, allowing us to get a quick overview of the data.\n",
    "\n",
    "df.info() provides details about the DataFrame, such as the number of entries, column data types, and non-null counts, helping us understand the dataset's structure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2382ba6a-7e67-466c-a8b7-95462471b641",
   "metadata": {},
   "source": [
    "# Step 4: Check for Missing Values\n",
    "Before preprocessing, we need to check for any missing values in the dataset.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7a672ebe-570e-4e36-a8b2-428e56d92bf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values in each column:\n",
      "feature_1     0\n",
      "feature_2     0\n",
      "feature_3     0\n",
      "feature_4     0\n",
      "feature_5     0\n",
      "feature_6     0\n",
      "feature_7     0\n",
      "feature_8     0\n",
      "feature_9     0\n",
      "feature_10    0\n",
      "feature_11    0\n",
      "feature_12    0\n",
      "feature_13    0\n",
      "feature_14    0\n",
      "feature_15    0\n",
      "feature_16    0\n",
      "feature_17    0\n",
      "feature_18    0\n",
      "feature_19    0\n",
      "feature_20    0\n",
      "feature_21    0\n",
      "feature_22    0\n",
      "feature_23    0\n",
      "feature_24    0\n",
      "feature_25    0\n",
      "feature_26    0\n",
      "feature_27    0\n",
      "feature_28    0\n",
      "feature_29    0\n",
      "feature_30    0\n",
      "feature_31    0\n",
      "feature_32    0\n",
      "feature_33    0\n",
      "feature_34    0\n",
      "feature_35    0\n",
      "feature_36    0\n",
      "feature_37    0\n",
      "feature_38    0\n",
      "feature_39    0\n",
      "feature_40    0\n",
      "feature_41    0\n",
      "feature_42    0\n",
      "feature_43    0\n",
      "feature_44    0\n",
      "feature_45    0\n",
      "feature_46    0\n",
      "feature_47    0\n",
      "feature_48    0\n",
      "feature_49    0\n",
      "feature_50    0\n",
      "feature_51    0\n",
      "feature_52    0\n",
      "feature_53    0\n",
      "feature_54    0\n",
      "feature_55    0\n",
      "feature_56    0\n",
      "feature_57    0\n",
      "spam_label    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"Missing values in each column:\")\n",
    "print(df.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb34b4e0-973e-44f2-a757-5cc1fb94de69",
   "metadata": {},
   "source": [
    "# Explanation:\n",
    "df.isnull().sum() counts the number of missing values in each column. Identifying missing values is crucial for appropriate data handling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31b266e9-d595-479a-ad88-4d2b640e973b",
   "metadata": {},
   "source": [
    "# Step 5: Handle Missing Values\n",
    "If there are missing values, we will handle them using imputation. Here, we will impute missing values with the mean for continuous features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "086b7912-5577-4b7b-8ccd-311173806007",
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer = SimpleImputer(strategy='mean')\n",
    "df.iloc[:, :-1] = imputer.fit_transform(df.iloc[:, :-1]) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76955963-039f-4945-9739-85df3d8988f1",
   "metadata": {},
   "source": [
    "# Explanation:\n",
    "> We create an instance of SimpleImputer, specifying the strategy as mean. This will replace missing values in continuous features with the mean of the respective columns.\n",
    "\n",
    "> df.iloc[:, :-1] selects all columns except the last one (the target variable) for imputation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72fbd816-fe52-4654-bcc3-ec3f1eb47551",
   "metadata": {},
   "source": [
    "# Step 6: Separate Features and Target Variable\n",
    "Now, we'll separate the features (X) from the target variable (y)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5af5ad82-3557-47db-9622-7cc31bebe73b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop('spam_label', axis=1)  # Features\n",
    "y = df['spam_label']  # Target variable\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92dd520b-a878-480a-bdc2-ba2267970950",
   "metadata": {},
   "source": [
    "# Explanation:\n",
    "X contains all the feature columns, while y contains the target column (spam_label). This separation is essential for training machine learning models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4563cc6-24dd-47e6-88e3-fafccfcb3ccd",
   "metadata": {},
   "source": [
    "# Step 7: Normalize the Feature Set\n",
    "Next, we'll normalize the feature set to scale the features to a range of [0, 1] using MinMaxScaler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "72726d50-96db-4415-88c6-fab5b60abd68",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "X_normalized = scaler.fit_transform(X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2850efc5-7607-42f3-b555-be0450cd320b",
   "metadata": {},
   "source": [
    "# Explanation:\n",
    "MinMaxScaler rescales the feature values to lie within a specified range (default is [0, 1]). Normalization is important for many machine learning algorithms as it ensures that features contribute equally to the result.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "155315ee-ac32-4818-9c95-2bb32ffceaad",
   "metadata": {},
   "source": [
    "# Step 8: Split the Data\n",
    "Finally, we'll split the dataset into training, validation, and test sets. This will help us evaluate our model effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0c11c875-3e17-42b9-bdda-212779d331ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set shape: (3220, 57)\n",
      "Validation set shape: (690, 57)\n",
      "Test set shape: (691, 57)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_temp, y_train, y_temp = train_test_split(X_normalized, y, test_size=0.3, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "print(f'Training set shape: {X_train.shape}')\n",
    "print(f'Validation set shape: {X_val.shape}')\n",
    "print(f'Test set shape: {X_test.shape}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a145964e-2f47-4624-9fe9-c43b809a5bfb",
   "metadata": {},
   "source": [
    "# Explanation:\n",
    ">We use train_test_split twice to create three subsets: training (70%), validation (15%), and test sets (15%).\n",
    ">\n",
    ">We display the shapes of the resulting datasets to confirm that the split was performed correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b039b7-84b8-4cef-ac39-cf00fa30b19e",
   "metadata": {},
   "source": [
    "# SUMMARY :\n",
    "In this tutorial, we walked through the preprocessing steps for the Spambase dataset from the UCI Machine Learning Repository, preparing it for machine learning tasks. We began by importing necessary libraries like pandas, numpy, and sklearn, followed by loading the dataset and assigning clear column names. An initial inspection of the dataset helped us understand its structure and check for missing values, which we addressed using the SimpleImputer by replacing them with the mean of the respective columns. We then separated the features from the target variable and normalized the feature set to a range of [0, 1] using MinMaxScaler, ensuring all features contribute equally to model training. Finally, the dataset was split into training (70%), validation (15%), and test sets (15%) to facilitate effective model evaluation. With these preprocessing steps completed, the dataset is now ready for training machine learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d05c99-80d8-4ba7-adc7-732883861e17",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
